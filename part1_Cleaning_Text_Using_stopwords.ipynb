{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part1 Using stopwords.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO86txZa5iLj3uVv4AwLl0g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Herzeg137/entering-to-NLP/blob/main/part1_Cleaning_Text_Using_stopwords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using the stopwords with NLTK library\n",
        "import nltk  #<<< NLTk ni import qilish\n",
        "from nltk.corpus import stopwords #<<< stopword larni chaqirish\n",
        "nltk.download('stopwords') #<<< Stopword larni yuklash\n",
        "nltk.download('punkt') #<<< punkt ni ishlatmasak ham bo'ladi lekin ba'zi paytlarda muommo chiqadi\n",
        "#aynan shuning uchun ishlatamiz\n",
        "#NOTE: tokenizer ni ishlatganimizda biz punkt ni yuklashimiz shart\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "print(stopwords.words('english')) #<<< Bu bizga Ingliz tilidagi stopword larni ko'rsatib beradi\n",
        "\n",
        "# random sentecnce with lot of stop words\n",
        "sample_text = \"Oh man, this is pretty cool. We will do more such things.\"\n",
        "text_tokens = word_tokenize(sample_text)\n",
        "\n",
        "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('english')]\n",
        "\n",
        "print(text_tokens) #<<< Bu esa gapni o'z holicha chiqarib beradi\n",
        "print(tokens_without_sw)  #<<<<< buning vazifasi bizga stopword larsiz hosil bo'lgan gapni ko'rsatish "
      ],
      "metadata": {
        "id": "u9Db4FUQP94I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using the stopwords with SpaCy library\n",
        "import spacy #<<< Chaqirish\n",
        "from nltk.tokenize import word_tokenize \n",
        "# loading english language model of spaCy\n",
        "en_model = spacy.load('en_core_web_sm')\n",
        "# gettign the list of default stop words in spaCy english model\n",
        "stopwords = en_model.Defaults.stop_words\n",
        "\n",
        "sample_text = \"Oh man, this is pretty cool. We will do more such things.\"\n",
        "text_tokens = word_tokenize(sample_text)\n",
        "tokens_without_sw= [word for word in text_tokens if not word in stopwords]\n",
        "\n",
        "print(text_tokens)\n",
        "print(tokens_without_sw)\n",
        "\n",
        "#NOTE: SpaCy va NLTK ni nima farqi bor? Ular bir xil faqat SpaCy da 326 ta stopwords bor, NLTK da esa 179 ta"
      ],
      "metadata": {
        "id": "-YOOOeriP-g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add custom stopwords to existed sws with NLTK library \n",
        "#step1 Import nltk and download stopwords, and then import stopwords from NLTK\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#step2 ets see the stop word list present in the NLTK library, without adding our custom list\n",
        "print(stopwords.words('english'))\n",
        "\n",
        "#step3 Create a Simple sentence\n",
        "simple_text = \"the city is beautiful, but due to traffic noice polution is increasing on daily basis which is hurting all the people\"\n",
        "\n",
        "#step4 Create our custom stopword list to add\n",
        "new_stopwords = [\"all\", \"due\", \"to\", \"on\", \"daily\"]\n",
        "\n",
        "#step5 add custom list to stopword list of nltk\n",
        "stpwrd = nltk.corpus.stopwords.words('english')\n",
        "stpwrd.extend(new_stopwords)\n",
        "\n",
        "#step6 download and import the tokenizer from nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#step7 tokenizing the simple text by using word tokenizer\n",
        "text_tokens = word_tokenize(simple_text)\n",
        "\n",
        "#step8 Remove the custom stop words and print it\n",
        "removing_custom_words = [words for words in text_tokens if not words in stpwrd]\n",
        "print(removing_custom_words)"
      ],
      "metadata": {
        "id": "tJYRrkDvgp4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NLTK ni Rus tilida foydalanish\n",
        "import nltk  \n",
        "from nltk.corpus import stopwords \n",
        "nltk.download('stopwords') \n",
        "nltk.download('punkt') \n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "print(stopwords.words('russian')) #<<< Bu yerni rus tiliga o'tqazamiz\n",
        "\n",
        "sample_text = \"О, чувак, это довольно круто. Мы будем делать больше таких вещей.\" #<<< bu yerdagi gapni rus tiliga o'giramiz\n",
        "text_tokens = word_tokenize(sample_text)\n",
        "\n",
        "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('russian')] #<<< bu yerni ham rus tili qilamiz\n",
        "\n",
        "print(text_tokens) \n",
        "print(tokens_without_sw)  "
      ],
      "metadata": {
        "id": "2fPShJ0ijOuB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}